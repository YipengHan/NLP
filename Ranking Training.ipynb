{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DiscoveryReady_logo.png](DiscoveryReady_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>General Ranking & Re-ranking</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3><center>Yipeng Han - 2018/07/17</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking project includes following steps:\n",
    "\n",
    "1. Data Loading & Cleaning\n",
    "2. Feature Engineering \n",
    "3. Models\n",
    "    * Logistic Regression\n",
    "    * Random Forest\n",
    "    * Neural Network\n",
    "    * Extreme Gradient Boosting\n",
    "    * Long Short Term Memory\n",
    "4. Ensemble and Stacking models\n",
    "5. Apply uncertainty sampling\n",
    "6. Save Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general ranking & re-ranking task, there is no specific code template right now. Mostly because input datasets have different format based on projects. So for here, I will use latest data on Stewart US as an example. Note that all those method, including both feature selecting, feature engineering and machine learning model may change more or less depending on cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:25:56.768950Z",
     "start_time": "2018-07-18T18:25:51.537688Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gc\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context('paper')\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "os.chdir('R:/Analytics & Automation/Machine Learning/Stewart Merger/US')\n",
    "mydata = pd.read_csv('20180709 ML Document Export US_export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:25:56.777944Z",
     "start_time": "2018-07-18T18:25:56.769949Z"
    }
   },
   "outputs": [],
   "source": [
    "mydata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform TFIDF into dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:25:59.772329Z",
     "start_time": "2018-07-18T18:25:56.779944Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = pd.read_csv('20180709 Stewart US Merger ML Export TFIDF Export.csv')\n",
    "del tfidf['workspacenum']\n",
    "tfidf.columns = ['Artifact ID','TFIDF']\n",
    "\n",
    "a = tfidf['TFIDF']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "count = Counter()\n",
    "TOP_TERMS = 50\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1\n",
    "\n",
    "tfidf_feature_list= [i for i, num in count.most_common(TOP_TERMS)]\n",
    "a = a.apply(lambda x: [('TFIDF_'+i) for i in x if i in tfidf_feature_list])\n",
    "a = a.apply(lambda x: ';'.join(x))\n",
    "a[a.str.len()==0] = 'TFIDF_nan'\n",
    "tfidf_feature = a.str.get_dummies(';')\n",
    "tfidf_feature = pd.concat([tfidf['Artifact ID'],tfidf_feature],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform cluster into dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:26:01.665594Z",
     "start_time": "2018-07-18T18:25:59.774329Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster = pd.read_csv('20180709 Stewart US Merger ML Export Lowest Level Relativity Analytics Cluster.csv')\n",
    "cluster = cluster[['ID','ClusterName']]\n",
    "cluster.columns = ['DocumentArtifactID','ClusterName']\n",
    "a = cluster['ClusterName']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "count = Counter()\n",
    "TOP_CLUSTER = 50\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1\n",
    "    \n",
    "cluster_feature_list= [i for i, num in count.most_common(TOP_CLUSTER)]\n",
    "a = a.apply(lambda x: [('Cluster_'+i) for i in x if i in cluster_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'Cluster_nan'\n",
    "cluster_feature = a.str.get_dummies(';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading decision: there are 3 sets of decision in Stewart case, which means to run the whole ranking part by 3 times based on different decision input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:26:02.006427Z",
     "start_time": "2018-07-18T18:26:01.665594Z"
    }
   },
   "outputs": [],
   "source": [
    "decision = pd.read_csv('20180709 Stewart Merge US ML Export -Three Set - Training Decisions.csv')\n",
    "decision = decision[['ArtifactID','SetNum','MajorCategory']]\n",
    "decision.columns =['Artifact ID','SetNum','Major Category']\n",
    "decision['Major Category'] = decision['Major Category'].replace('Needs Further Review','Responsive')\n",
    "decision['Major Category'] = decision['Major Category'].replace('Technical Unresolved ','Not Responsive')\n",
    "decision1 = decision[decision['SetNum']==1][['Artifact ID','Major Category']]\n",
    "decision2 = decision[decision['SetNum']==2][['Artifact ID','Major Category']]\n",
    "decision3 = decision[decision['SetNum']==3][['Artifact ID','Major Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create features for other columns from main table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:26:12.334072Z",
     "start_time": "2018-07-18T18:26:02.008426Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "STR_NUMBER = 40\n",
    "a = mydata['STR - 20180618 US Search Terms 001 W/10']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "a = a.apply(lambda x: ['STR_w10_001'+ i for i in x])\n",
    "count = Counter()\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1    \n",
    "w10_001_feature_list= [i for i, num in count.most_common(STR_NUMBER)]\n",
    "a = a.apply(lambda x: [i for i in x if i in w10_001_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'STR_w10_001_nan'\n",
    "w10_001_feature = a.str.get_dummies(',')\n",
    "\n",
    "a = mydata['STR - 20180618 US Search Terms 001 W/5']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "a = a.apply(lambda x: ['STR_w5_001'+ i for i in x])\n",
    "count = Counter()\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1    \n",
    "w5_001_feature_list= [i for i, num in count.most_common(STR_NUMBER)]\n",
    "a = a.apply(lambda x: [i for i in x if i in w5_001_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'STR_w5_001_nan'\n",
    "w5_001_feature = a.str.get_dummies(',')\n",
    "\n",
    "a = mydata['STR - 20180618 US Search Terms 002 W/10']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "a = a.apply(lambda x: ['STR_w10_002'+ i for i in x])\n",
    "count = Counter()\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1    \n",
    "w10_002_feature_list= [i for i, num in count.most_common(STR_NUMBER)]\n",
    "a = a.apply(lambda x: [i for i in x if i in w10_002_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'STR_w10_002_nan'\n",
    "w10_002_feature = a.str.get_dummies(',')\n",
    "\n",
    "\n",
    "a = mydata['STR - 20180618 US Search Terms 002 W/5']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "a = a.apply(lambda x: ['STR_w5_002'+ i for i in x])\n",
    "count = Counter()\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1    \n",
    "w5_002_feature_list= [i for i, num in count.most_common(STR_NUMBER)]\n",
    "a = a.apply(lambda x: [i for i in x if i in w5_002_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'STR_w5_002_nan'\n",
    "w5_002_feature = a.str.get_dummies(',')\n",
    "\n",
    "\n",
    "a = mydata['STR - 20180619 US Search Terms 003 W/10']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "a = a.apply(lambda x: ['STR_w10_003'+ i for i in x])\n",
    "count = Counter()\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1    \n",
    "w10_003_feature_list= [i for i, num in count.most_common(STR_NUMBER)]\n",
    "a = a.apply(lambda x: [i for i in x if i in w10_003_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'STR_w10_003_nan'\n",
    "w10_003_feature = a.str.get_dummies(',')\n",
    "\n",
    "a = mydata['STR - 20180619 US Search Terms 003 W/5']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "a = a.apply(lambda x: ['STR_w5_002'+ i for i in x])\n",
    "count = Counter()\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1    \n",
    "w5_003_feature_list= [i for i, num in count.most_common(STR_NUMBER)]\n",
    "a = a.apply(lambda x: [i for i in x if i in w5_003_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'STR_w5_003_nan'\n",
    "w5_003_feature = a.str.get_dummies(',')\n",
    "\n",
    "\n",
    "\n",
    "a = mydata['STR - DONOTUSE zAA Keyword Mining 001']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "count = Counter()\n",
    "TOP_TERMS = 50\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1\n",
    "\n",
    "zAA_feature_list= [i for i, num in count.most_common(TOP_TERMS)]\n",
    "a = a.apply(lambda x: [('zAA_'+i) for i in x if i in zAA_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'zAA_nan'\n",
    "zAA_feature = a.str.get_dummies(',')\n",
    "\n",
    "a = mydata['STR - DR Responsive - US (Text)']\n",
    "a = a.apply(lambda x: str(x).split(';'))\n",
    "count = Counter()\n",
    "TOP_TERMS = 50\n",
    "for coord in a:\n",
    "    count[coord[0]] += 1\n",
    "\n",
    "Analytics_Res_feature_list= [i for i, num in count.most_common(TOP_TERMS)]\n",
    "a = a.apply(lambda x: [('DRres_'+i) for i in x if i in Analytics_Res_feature_list])\n",
    "a = a.apply(lambda x: ','.join(x))\n",
    "a[a.str.len()==0] = 'DRres_nan'\n",
    "DR_Res_feature = a.str.get_dummies(',')\n",
    "\n",
    "SENDER_NUMBER = 15\n",
    "SENDER_list = mydata['Sender Domains'].value_counts().index[0:(SENDER_NUMBER+1)]\n",
    "mydata['Sender Domains'] = mydata['Sender Domains'].apply(lambda x: \n",
    "    x if x in SENDER_list else 'Sender: nan')\n",
    "Sender_domain_feature = mydata['Sender Domains'].str.get_dummies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all columns has been transformed into dummy for future usage. And there comes some basic feature engineering (transforming + normalization) to make sure that the following ML methods run smoothly. Need double check all features when initiating ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:26:13.332045Z",
     "start_time": "2018-07-18T18:26:12.336072Z"
    }
   },
   "outputs": [],
   "source": [
    "mydata['DocType'] = mydata['DocType'].apply(lambda x: 'Doctype_'+str(x))\n",
    "DocType = mydata['DocType'].str.get_dummies() \n",
    "\n",
    "mydata['ParentDate'] = mydata['ParentDate'].fillna('01/01/1990')\n",
    "mydata['ParentDate'] = mydata['ParentDate'].apply(lambda x: (datetime(2018,4,1,0,0) - \n",
    "      datetime.strptime(str(x),'%m/%d/%Y')).days)\n",
    "mydata['isParent'] = mydata['isParent'].replace(True,1)\n",
    "mydata['isParent'] = mydata['isParent'].replace(False,0)\n",
    "mydata['IsEmbedded'] = mydata['IsEmbedded'].replace(True,1)\n",
    "mydata['IsEmbedded'] = mydata['IsEmbedded'].replace(False,0)\n",
    "mydata['IsEmbedded']  = mydata['IsEmbedded'].fillna(0)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "mydata = mydata.drop(['BegDoc','BegAttach' ,'DR ITR Category','DocTypeDesc','DocType','DateSent', 'DateLastMod','Sender Domains'\n",
    "                      ,'FileTypeDesc','FileExt',\n",
    "                      'STR - 20180618 US Search Terms 001 W/10',\n",
    "       'STR - 20180618 US Search Terms 001 W/5',\n",
    "       'STR - 20180618 US Search Terms 002 W/10',\n",
    "       'STR - 20180618 US Search Terms 002 W/5',\n",
    "       'STR - 20180619 US Search Terms 003 W/10',\n",
    "       'STR - 20180619 US Search Terms 003 W/5',\n",
    "       'STR - DONOTUSE zAA Keyword Mining 001',\n",
    "       'STR - DR Responsive - US (Text)'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:26:14.351905Z",
     "start_time": "2018-07-18T18:26:13.332045Z"
    }
   },
   "outputs": [],
   "source": [
    "mydata = pd.merge(mydata,tfidf_feature,on='Artifact ID',how='left')\n",
    "mydata = pd.merge(mydata,cluster_feature,left_on='Artifact ID',right_on='DocumentArtifactID',how='left')\n",
    "mydata = pd.concat([mydata,DocType,w10_001_feature,w5_001_feature,w10_002_feature,w5_002_feature,\n",
    "                    w10_003_feature,w5_003_feature,Sender_domain_feature,zAA_feature,DR_Res_feature],axis=1)\n",
    "\n",
    "mydata['ExtractedTextSize']=mydata['ExtractedTextSize'].fillna(0)\n",
    "mydata['ExtractedTextSize'] = preprocessing.scale(mydata['ExtractedTextSize'])\n",
    "mydata['FileSize'] = mydata['FileSize'].fillna(0)\n",
    "mydata['FileSize'] = preprocessing.scale(mydata['FileSize'])\n",
    "mydata['ParentDate'] = mydata['ParentDate'].fillna(0)\n",
    "mydata['ParentDate'] = preprocessing.scale(mydata['ParentDate'])\n",
    "mydata['NumAttach'] = mydata['NumAttach'].fillna(0)\n",
    "mydata['NumAttach'] = preprocessing.scale(mydata['NumAttach'])\n",
    "mydata['Recipients_Count']= preprocessing.scale(mydata['Recipients_Count'])\n",
    "gc.collect()\n",
    "b = mydata.copy()\n",
    "mydata = b.copy()\n",
    "mydata = pd.merge(mydata,decision3,left_on = 'Artifact ID',right_on='Artifact ID',how = 'left')\n",
    "mydata['Major Category'] = mydata['Major Category'].replace('Responsive',1)\n",
    "mydata['Major Category'] = mydata['Major Category'].replace('Not Responsive',0)\n",
    "mydata['Major Category'] = mydata['Major Category'].replace('Technical Issue',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T18:26:14.352904Z",
     "start_time": "2018-07-18T18:25:51.606Z"
    }
   },
   "outputs": [],
   "source": [
    "train = mydata[mydata['Major Category'].isin([0,1])].fillna(0)\n",
    "\n",
    "y = train['Major Category']\n",
    "print('Richnessof  of this data set: %2.2f' % (sum(y)/len(y)))\n",
    "train = train.drop(['Major Category'],axis=1)\n",
    "x = train.copy()\n",
    "x = x.fillna(0)\n",
    "y = y.fillna(0)\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['ArtifactID']=mydata['Artifact ID']\n",
    "mydata_temp = mydata.drop(['Artifact ID'],axis=1).fillna(0)\n",
    "x_predict = pd.DataFrame()\n",
    "x_predict['BegDoc'] = train['Artifact ID']\n",
    "del train['Artifact ID']\n",
    "\n",
    "del x['Artifact ID']\n",
    "del mydata_temp['Major Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we are using SMOTE to oversample the minority and truncated SVD to create some extra new features (most likely our raw data is sparse because we generate too many string dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:25:10.203497Z",
     "start_time": "2018-07-27T17:24:53.152988Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "resampler = SMOTETomek(ratio = 'auto', smote = SMOTE(ratio='auto',k_neighbors=5,kind='svm'))\n",
    "#resampler = RandomUnderSampler()\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "K = 200\n",
    "svd = TruncatedSVD(n_components = K,random_state=0)\n",
    "SVD_x = svd.fit(x)\n",
    "x = pd.concat([x,pd.DataFrame(SVD_x.transform(x))],axis=1)\n",
    "mydata_temp =  pd.concat([mydata_temp,pd.DataFrame(SVD_x.transform(mydata_temp))],axis=1)\n",
    "perc=SVD_x.explained_variance_ratio_.cumsum()[K-1]\n",
    "print(\"Cumulative explained variation: \" + \"{:.2%}\".format(perc))\n",
    "# set up for gridsearch\n",
    "prec_scorer = make_scorer(precision_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All ML models follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RFE(recursive feature selection) to find the best features\n",
    "2. GridSearch to find to optimal set of parameters\n",
    "3. SMOTE for training set to make it balanced\n",
    "4. Fitting data to ML model\n",
    "5. Make classification report and ROC curve plot\n",
    "6. Do prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "starttime = time()\n",
    "rfecv = RFECV(estimator=LogisticRegression(), step=0.02, cv=5,verbose=1, scoring='roc_auc')\n",
    "rfecv.fit(x, y)\n",
    "endtime = time()\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "#print('Selected features: %s' % list(x.columns[rfecv.support_]))\n",
    "print('Running time: ',(endtime-starttime),'s')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Logistic regression RFE result')\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (number of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "#log_selected_feature = list(x.columns[rfecv.support_])\n",
    "log_x = x.iloc[:,rfecv.support_]\n",
    "x_train, x_test, y_train, y_test = train_test_split(log_x, y, test_size=0.3, random_state=5)\n",
    "x_train ,y_train = resampler.fit_sample(x_train, y_train)\n",
    "logit = LogisticRegression()\n",
    "parameters={'penalty':['l1','l2'],'C':[1,0.01,0.001,10,0.1],'random_state':[0,5,10,20]}\n",
    "grid = GridSearchCV(estimator= logit,param_grid=parameters,cv=5,refit=True, scoring=prec_scorer)\n",
    "grid = grid.fit(x_train,y_train)\n",
    "logit = grid.best_estimator_\n",
    "print(grid.best_params_ )\n",
    "print(grid.best_score_)\n",
    "logit.fit(x_train,y_train)\n",
    "\n",
    "y_pred = logit.predict(x_test)\n",
    "y_pred_proba = logit.predict_proba(x_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba,pos_label=1)\n",
    "print('Train/Test split results:')\n",
    "print(logit.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "print(logit.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\n",
    "print(logit.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n",
    "log_confusion_matrix1 = confusion_matrix(y_test, y_pred)\n",
    "print(log_confusion_matrix1)\n",
    "classification_report1 = classification_report(y_test, y_pred,digits=3)\n",
    "print(classification_report1)\n",
    "print('Logistic Regression model has out of sample accuracy: %.3f'%(100*np.sum(y_pred == y_test)/len(y_test)),'%')\n",
    "\n",
    "#\n",
    "#idx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n",
    "## plot ROC curve\n",
    "#plt.figure(figsize=(10,6))\n",
    "#plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "#plt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\n",
    "#plt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\n",
    "#plt.xlim([0.0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "#plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "#plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "#plt.title('Logistic Regression Receiver operating characteristic (ROC) curve')\n",
    "#plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "temp = logit.predict_proba(mydata_temp.iloc[:,rfecv.support_])[:,1]\n",
    "result['LR'] = temp\n",
    "x_predict['LR'] = logit.predict_proba(log_x)[:, 1]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfecv2 = RFECV(RandomForestClassifier(), step=1, cv=3,verbose=1, scoring='accuracy')\n",
    "rfecv2.fit(x, y)\n",
    "print(\"Optimal number of features: %d\" % rfecv2.n_features_)\n",
    "#print('Selected features: %s' % list(x.columns[rfecv2.support_]))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Random Forest RFE result')\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (number of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv2.grid_scores_) + 1), rfecv2.grid_scores_)\n",
    "plt.show()\n",
    "#\n",
    "#forest_selected_feature = list(x.columns[rfecv2.support_])\n",
    "forest_x = x.iloc[:,rfecv2.support_]\n",
    "x_train, x_test, y_train, y_test = train_test_split(forest_x, y, test_size=0.3, random_state=40)\n",
    "x_train,y_train = resampler.fit_sample(x_train,y_train)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'n_estimators': [4, 9, 25, 100], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 5, 10], \n",
    "              'min_samples_split': [2, 5],\n",
    "              'min_samples_leaf': [1, 5, 10],\n",
    "              'oob_score': [True, False]\n",
    "             }\n",
    "grid = GridSearchCV(estimator= rf,param_grid=parameters,cv=5, scoring=prec_scorer,verbose=1)\n",
    "grid = grid.fit(x_train,y_train)\n",
    "rf = grid.best_estimator_\n",
    "print(grid.best_params_ )\n",
    "print(grid.best_score_)\n",
    "rf.fit(x_train,y_train)\n",
    "\n",
    "y_pred_3 = rf.predict(x_test).round()\n",
    "y_pred_proba_3 = rf.predict_proba(x_test)[:,1]\n",
    "forest_confusion_matrix = confusion_matrix(y_test, y_pred_3)\n",
    "print(forest_confusion_matrix)\n",
    "print('Random Forest model has out of sample accuracy of %.3f' %(100*np.sum(y_pred_3 == y_test)/len(y_test)),'%')\n",
    "\n",
    "\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba_3)\n",
    "print('Train/Test split results:')\n",
    "print(rf.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred_3))\n",
    "print(rf.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba_3))\n",
    "print(rf.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n",
    "rf_confusion_matrix2 = confusion_matrix(y_test, y_pred_3)\n",
    "print(rf_confusion_matrix2)\n",
    "classification_report2 = classification_report(y_test, y_pred_3,digits=3)\n",
    "print(classification_report2)\n",
    "print('Random Forest model has out of sample accuracy: %.3f'%(100*np.sum(y_pred_3== y_test)/len(y_test)),'%')\n",
    "\n",
    "#idx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n",
    "## plot ROC curve\n",
    "#plt.figure(figsize=(6,6))\n",
    "#plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "#plt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\n",
    "#plt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\n",
    "#plt.xlim([0.0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "#plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "#plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "#plt.title('Random Forest Receiver operating characteristic (ROC) curve')\n",
    "#plt.legend(loc=\"lower right\")\n",
    "##plt.savefig('Graphs/RF - ROC.png',dpi=1200)\n",
    "#plt.show()\n",
    "\n",
    "temp = rf.predict_proba(mydata_temp.iloc[:,rfecv2.support_])\n",
    "result['RF'] = temp[:,1]\n",
    "x_predict['RF'] = rf.predict_proba(forest_x)[:, 1]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import adam, Adadelta\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)\n",
    "x_train ,y_train = resampler.fit_sample(x_train, y_train)\n",
    "\n",
    "NN = Sequential()\n",
    "NN.add(Dense(64, input_dim=x_train.shape[1], activation='relu', kernel_initializer  = RandomNormal(mean=0.0, stddev=1/64, seed=None)))\n",
    "#NN.add(Dropout(0.1))\n",
    "NN.add(Dense(64,  activation='relu', kernel_initializer  = RandomNormal(mean=0.0, stddev=2/64, seed=None)))\n",
    "#NN.add(Dropout(0.1))\n",
    "NN.add(Dense(32,  activation='relu'))\n",
    "NN.add(Dense(32,  activation='relu'))\n",
    "NN.add(Dense(32,  activation='relu'))\n",
    "#NN.add(Dropout(0.1))\n",
    "##NN.add(Dropout(0.2))\n",
    "#NN.add(Dense(64,  activation='relu', kernel_initializer  = RandomNormal(mean=0.0, stddev=2/256, seed=None)))\n",
    "#NN.add(Dense(32,  activation='relu', kernel_initializer  = RandomNormal(mean=0.0, stddev=2/256, seed=None)))\n",
    "NN.add(Dense(1,activation='sigmoid'))\n",
    "opt = adam(lr = 0.01)\n",
    "NN.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy',recall])\n",
    "NN.summary()\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_recall', patience=90,verbose=1, mode='auto')\n",
    "ReduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.5,verbose=1,patience=30)\n",
    "#path = 'ModelCheckPoint/weights.{epoch:02d} -- Loss.{val_loss:.2f} -- Recall.{val_recall:.2f}.hdf5'\n",
    "#Checkpoint = ModelCheckpoint(path,monitor='val_recall',verbose=1,save_best_only=True,mode = 'val_recall')\n",
    "callbacks_list = [ReduceLR]\n",
    "NN.fit(x_train,y_train, epochs=100, batch_size=10,verbose=1,validation_data=(x_test, y_test),\n",
    "          shuffle=True,callbacks=callbacks_list)\n",
    "\n",
    "# callbacks=callbacks_list\n",
    "#callbacks = callbacks_list_test\n",
    "y_nn_pred_prob = NN.predict_proba(x_test)\n",
    "y_nn_pred =  NN.predict_classes(x_test).squeeze()\n",
    "\n",
    "[fpr_nn, tpr_nn, thr_nn] = roc_curve(y_test, y_nn_pred_prob)\n",
    "print('Train/Test split results:')\n",
    "print(NN.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_nn_pred))\n",
    "print(NN.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_nn_pred))\n",
    "print(NN.__class__.__name__+\" auc is %2.3f\" % auc(fpr_nn, tpr_nn))\n",
    "\n",
    "nn_confusion_matrix = confusion_matrix(y_test, y_nn_pred)\n",
    "print(nn_confusion_matrix)\n",
    "classification_report4 = classification_report(y_test, y_nn_pred,digits=3)\n",
    "print(classification_report4)\n",
    "print('Neural Network model has out of sample accuracy of %.3f' %(100*np.sum(y_nn_pred == y_test)/len(y_test)),'%')\n",
    "\n",
    "idx = np.min(np.where(tpr_nn > 0.95))\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr_nn, tpr_nn, color='coral', label='ROC curve after SMOTE (area = %0.3f, recall = %0.3f)' % (auc(fpr_nn, tpr_nn),float(classification_report4.split()[11])))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot([0,fpr_nn[idx]], [tpr_nn[idx],tpr_nn[idx]], 'k--', color='blue')\n",
    "plt.plot([fpr_nn[idx],fpr_nn[idx]], [0,tpr_nn[idx]], 'k--', color='blue')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "plt.title('Neural Network - ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Graphs/NN - ROC.png',dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "temp = NN.predict_proba(mydata_temp)\n",
    "result['Neural Network'] = temp\n",
    "x_predict['Neural Network'] = NN.predict_proba(x)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=5)\n",
    "x_train ,y_train = resampler.fit_sample(x_train, y_train)\n",
    "\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "parameters = {'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05,0.1,0.001],\n",
    "              'max_depth': [5,20],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.6,0.8],\n",
    "              'n_estimators': [5,20,100]}\n",
    "grid = GridSearchCV(estimator= xgb,param_grid=parameters,cv=5, scoring=prec_scorer,verbose=1)\n",
    "grid = grid.fit(x_train,y_train)\n",
    "xgb = grid.best_estimator_\n",
    "print(grid.best_params_ )\n",
    "print(grid.best_score_)\n",
    "xgb.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "y_xgb_pred_prob = xgb.predict_proba(x_test)[:, 1]\n",
    "y_xgb_pred =  xgb.predict(x_test).squeeze()\n",
    "\n",
    "[fpr_xgb, tpr_xgb, thr_xgb] = roc_curve(y_test, y_xgb_pred_prob)\n",
    "print('Train/Test split results:')\n",
    "print(xgb.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_xgb_pred))\n",
    "print(xgb.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_xgb_pred))\n",
    "print(xgb.__class__.__name__+\" auc is %2.3f\" % auc(fpr_xgb, tpr_xgb))\n",
    "\n",
    "xgb_confusion_matrix = confusion_matrix(y_test, y_xgb_pred)\n",
    "print(xgb_confusion_matrix)\n",
    "classification_report5 = classification_report(y_test, y_xgb_pred,digits=3)\n",
    "print(classification_report5)\n",
    "\n",
    "\n",
    "temp = xgb.predict_proba(mydata_temp)[:, 1]\n",
    "result['XGBoost'] = temp\n",
    "x_predict['XGBoost'] = xgb.predict_proba(x)[:, 1]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading LSTM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add LSTM result\n",
    "#os.chdir('R:/Analytics & Automation/Machine Learning/Stewart Merger/US')\n",
    "#lstm = pd.read_csv('Stewart US LSTM 20180709.csv')\n",
    "#del lstm['Unnamed: 0']\n",
    "#lstm.columns=['ArtifactID','LSTM']\n",
    "#\n",
    "#ids = lstm['ArtifactID']\n",
    "#lstm = lstm[~ids.isin(ids[ids.duplicated()])]\n",
    "#result = pd.merge(result,lstm,on='ArtifactID',how='left').fillna(0.5)\n",
    "#x_predict = pd.merge(x_predict,lstm,on='ArtifactID',how='left').fillna(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Ensemble'] = ((result['LR']+result['RF']+result['Neural Network']+result['XGBoost'])/4)\n",
    "x_predict['Ensemble'] = ((x_predict['LR']+x_predict['RF']+x_predict['Neural Network']+result['XGBoost'])/4)\n",
    "\n",
    "\n",
    "y_ensemble_pred_prob = x_predict['Ensemble']\n",
    "y_ensemble_pred =  x_predict['Ensemble'].round().squeeze()\n",
    "\n",
    "y_pre = [int(item>0.5) for  item in y_ensemble_pred_prob]\n",
    "[fpr, tpr, thr] = roc_curve(y, y_pre)\n",
    "print('Train/Test split results:')\n",
    "print(\"Ensemble model accuracy is %2.3f\" % accuracy_score(y, y_pre))\n",
    "print(\"Ensemble model log_loss is %2.3f\" % log_loss(y,y_pre))\n",
    "print(\"Ensemble model auc is %2.3f\" % auc(fpr, tpr))\n",
    "\n",
    "ensemble_confusion_matrix = confusion_matrix(y,y_pre)\n",
    "print(ensemble_confusion_matrix)\n",
    "classification_report4 = classification_report(y, y_pre,digits=3)\n",
    "print(classification_report4)\n",
    "print('Ensemble model has out of sample accuracy of %.3f' %(100*np.sum(y_pre == y)/len(y)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_predict.iloc[:,1:5], y, test_size=0.3, random_state=40)\n",
    "stack_lr = LogisticRegression()\n",
    "stack_lr.fit(x_train,y_train)\n",
    "\n",
    "y_pred = stack_lr.predict(x_test)\n",
    "y_pred_proba = stack_lr.predict_proba(x_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "print('Train/Test split results:')\n",
    "print(stack_lr.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "print(stack_lr.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\n",
    "print(stack_lr.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n",
    "stack_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(stack_confusion_matrix)\n",
    "classification_report3 = classification_report(y_test, y_pred,digits=3)\n",
    "print(classification_report3)\n",
    "print('Stacking Logistic Regression model has out of sample accuracy: %.3f'%(100*np.sum(y_pred == y_test)/len(y_test)),'%')\n",
    "#\n",
    "#idx = np.min(np.where(tpr > 0.95))\n",
    "#plt.figure(figsize=(6,6))\n",
    "#plt.plot(fpr, tpr, color='coral', label='ROC curve after SMOTE (area = %0.3f, recall = %0.3f)' % (auc(fpr, tpr),float(classification_report3.split()[11])))\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "#plt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\n",
    "#plt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\n",
    "#plt.xlim([0.0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "#plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "#plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "#plt.title('Stacking - ROC curve')\n",
    "#plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('Graphs/Stacking - ROC.png',dpi=1200)\n",
    "#plt.show()\n",
    "\n",
    "result['Stack logistic'] = stack_lr.predict_proba(result.iloc[:,1:5])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the ML part for 3 times, we need to combine the result from these 3 sets and apply uncertainty sampling to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T18:34:28.718005Z",
     "start_time": "2018-07-27T18:34:28.701016Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('R:/Analytics & Automation/Machine Learning/Stewart Merger/US Final Pred Coding/20180720 Rank')\n",
    "result.to_csv('US Result set1 20180720.csv')\n",
    "\n",
    "set1 = pd.read_csv('US Result set1 20180720.csv')\n",
    "set2 = pd.read_csv('US Result set2 20180720.csv')\n",
    "set3 = pd.read_csv('US Result set3 20180720.csv')\n",
    "\n",
    "set4 = (set1+set2+set3)/3\n",
    "result=set4\n",
    "del result['Unnamed: 0']\n",
    "\n",
    "os.chdir('R://Analytics & Automation//Machine Learning//Yipeng//Uncertainty Sampling')\n",
    "import US\n",
    "\n",
    "probs = pd.DataFrame(result['Stack logistic'])\n",
    "index =  US.uncertainty_sampling('least_confident', probs)\n",
    "result = result.ix[index]\n",
    "result = result.reset_index().reset_index()\n",
    "result = result.rename(columns={'level_0':'Review Order'})\n",
    "result = result.set_index('index')\n",
    "result = result.sort_index()\n",
    "os.chdir('R:/Analytics & Automation/Machine Learning/Stewart Merger/US Final Pred Coding/20180720 Rank')\n",
    "result.to_csv('US Result combined 20180719.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "504px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "372px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
